---
output: html_document
---

<body id="start">
<div class="topnav">
  <a href='index.html#Frontpage'>Frontpage</a>
  <a href='data-visualisation.html#data-visualisation'>Data visualisation</a>
  <a href='parametized-data-germany.html#parametized-data'>Parametizing data</a>
  <a href='directory-structure.html#directory-structure'>Directory structure</a>
  <a href='creating-a-r-package.html#creating-a-r-package'>R-package</a>
  <a href='sql.html#SQL'>SQL</a>
  <a href='bibliography-using-zotero.html#Bibliography using Zotero'>Zotero</a>
  <a href='open-reproductibility-analysis.html#open-reproductibility-analysis'>Reproductibility</a>
  <a href='future-endeavours.html#future-endeavours'>Future endeavours</a>
  <a href='new-skills.html#new-skills'> New skills (Machine learning)</a>
  <a href='cv.html#cv'>CV</a>
  <a href='bibliography.html#bibliography'>Bibliography</a>
</div>

```{r, include=FALSE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
```

&nbsp;

For the studying of pre-processing, we'll use the package called "Karet". caret, short for Classification And REgression Training, is a package containing functions to help create machine learning-based (or actually, any form of predictive) models.

```{r}
library(caret)
names(getModelInfo()) %>% head(50)
```

Caret contains _a lot_ of predictive models: above are shown only the top 50 of the over 200 models available in caret. For now we'll continue to use a KNN model, though we'll study another model later too.

First of all, we can use "createDataPartition" instead of "sample" to create a training and a test group more easily

```{r}
set.seed(1234)
index<-createDataPartition(iris$Species, p=0.75, list=FALSE)

iris_training2<-iris[index,]

iris_test2<-iris[-index,]
```

Now, with the training data separated, we can use the caret-method of training a model and predicting with it

```{r}
model_knn<-train(iris_training2[,1:4], iris_training2[,5], method='knn')

#Then, we use the model to make a prediction
prediction<-predict(object=model_knn, iris_test2[,1:4])

#We check if the predicitons are true
pred<-prediction == iris_test2[,5]
table(pred)
```

And, due to the fact that we're using caret, we can create a confusion matrix: this'll allow us to study the sensitivity and the specificity of our model way easier.

```{r}
#And we create a confusion matrix for the results
confusionMatrix(prediction,iris_test2[,5])
```

We can see that the model can identify Setosa perfectly, but struggles a little with versicolor (92% accuracy) and some more with virginica (83% accuracy).

What's nice about caret training is that it contains pre-built in preprocessing methods. For the most basic examples of pre-processing, take centering and scaling.

- Centering is a form of pre-processing where the mean of the data is determined, and all other data is represented based on their distance to the mean

- Scaling is a form of pre-processing where all data is changed to the same scale of size. This makes sure the machine learning algorythm does not determine that one variable is more important than another, purely because it has higher numbers.

```{r}
model_knn<-train(iris_training2[,1:4],iris_training2[,5],method='knn',preProcess = c("center", "scale"))

prediction<-predict(object=model_knn, iris_test2[,1:4], type='raw')

pred<-prediction == iris_test2[,5]
table(pred)

confusionMatrix(prediction, iris_test2[,5])
```

centering and scaling the data already increases the accuracy of the system, making it detect the versicolor originally classified as a virginica as the correct species. To further explore the power of pre-processing, we'll take our home-made Glass KNN-model again, re-make it in caret, and pre-process the data before applying the model.

```{r}
library(mlbench)
data(Glass)

set.seed(4321)
ind_glass<-sample(2, nrow(Glass), replace=TRUE, prob=c(0.67,0.33)) #Creating a random selection of datapoints

glass_training<-Glass[ind_glass==1,1:9] # Separating the training the training and the testing datasets
glass_test<-Glass[ind_glass==2,1:9]

glass_training_labels<-Glass[ind_glass==1,10] # Storing the labels seperately
glass_test_labels<-Glass[ind_glass==2,10]

model_knn_glass<-train(glass_training, glass_training_labels, method="knn", preProcess=c("center", "scale"))

prediction<-predict(object=model_knn_glass, glass_test, type="raw")

confusionMatrix(prediction, glass_test_labels)
```

As you can see, pre-processing does not significantly affect the accuracy of the model. In fact, if we compare the confusionMatrix of this model to the original model set up using the KNN: command, you'll see that creating the model this way actually _reduced_ it's accuracy.

```{r}
set.seed(4321)
ind_glass<-sample(2, nrow(Glass), replace=TRUE, prob=c(0.67,0.33)) #Creating a random selection of datapoints

Glass_norm<-normalize(Glass[1:9]) %>% mutate(Type=Glass$Type)

glass_training<-Glass_norm[ind_glass==1,1:9] # Separating the training the training and the testing datasets
glass_test<-Glass_norm[ind_glass==2,1:9]

glass_training_labels<-Glass_norm[ind_glass==1,10] # Storing the labels seperately
glass_test_labels<-Glass_norm[ind_glass==2,10]

glass_pred<-class::knn(glass_training, glass_test, glass_training_labels) #Performing the machine learning test.

confusionMatrix(glass_test_labels, glass_pred)
```

Why this happens is something I've yet to figure out: perhaps the class::knn model already uses a form of built-in preprocessing. However, the KNN-model was purely used as an introduction to other machine learning processes, the question about why the KNN-model loses accuracy using karet is one for another time. For now, we'll continue to the next machine learning technique, one significant in the field of microbiology: randomForest.

